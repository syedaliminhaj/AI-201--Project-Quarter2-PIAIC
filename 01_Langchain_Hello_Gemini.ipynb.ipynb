{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "w7tAER_nE6gs"
      },
      "outputs": [],
      "source": [
        "!pip install langchain -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai -q"
      ],
      "metadata": {
        "id": "BZIlf64QGIYM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xek7C18kGOyH",
        "outputId": "8870ce56-0949-4dc1-f4a6-3eeb84d86342"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsb-4n-lGUs-",
        "outputId": "633214f6-7d64-48cb-8515-2219ba5bee6e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import google.generativeai as genai\n",
        "from langchain.llms.base import LLM\n",
        "from typing import Optional, List, Mapping, Any"
      ],
      "metadata": {
        "id": "PPPSZxOuKXny"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FCUTXpbJGaZW",
        "outputId": "c5096197-8be1-4fb4-dbd4-3df2c5d001bd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyAbafLmfBnbkWXc3R6H0DQMhtmgZIWBaFk'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError(\"API key not found. Make sure 'GOOGLE_API_KEY' is set in userdata.\")\n",
        "\n",
        "genai.configure(api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "grC52WXgGpxV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeminiLLM(LLM):\n",
        "    \"\"\"Google Gemini LLM.\"\"\"\n",
        "\n",
        "    model: str = \"gemini-pro\"\n",
        "    temperature: float = 0.7\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"google_gemini\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        model = genai.GenerativeModel(self.model)\n",
        "        response = model.generate_content(prompt, generation_config={\"temperature\": self.temperature})\n",
        "        return response.text\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\"model\": self.model, \"temperature\": self.temperature}"
      ],
      "metadata": {
        "id": "LzqjwabMGySK"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = GeminiLLM(model=\"gemini-pro\", temperature=0.7)\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Did2FTQ5KyiH",
        "outputId": "0aeb8c21-b707-4336-b8cf-13410d325c3c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-0eee168035d3>:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt_template)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"define laptop?\"\n",
        "response = chain.run({\"question\": question})\n",
        "\n",
        "print(\"Answer:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "VEOIVTFFK3tl",
        "outputId": "e9713a92-89ac-4bd4-8c8f-8b4d82b0c754"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: **Definition of Laptop:**\n",
            "\n",
            "A laptop, also known as a notebook computer, is a portable personal computer designed for mobile use. It is typically smaller and lighter than a desktop computer, and it incorporates a built-in display, keyboard, and touchpad.\n",
            "\n",
            "**Key Features of a Laptop:**\n",
            "\n",
            "* **Portability:** Laptops are designed to be easily carried and used on the go.\n",
            "* **All-in-one design:** They integrate the essential components of a desktop computer into a single device.\n",
            "* **Wireless connectivity:** Most laptops have built-in Wi-Fi and Bluetooth for connecting to the internet and other devices.\n",
            "* **Battery-powered:** Laptops are equipped with rechargeable batteries that allow for several hours of use without being plugged in.\n",
            "* **Compact size:** Laptops are typically smaller than desktop computers, with screens ranging from 11 to 17 inches.\n",
            "\n",
            "**Uses of Laptops:**\n",
            "\n",
            "Laptops are used for a wide range of tasks, including:\n",
            "\n",
            "* Productivity: document creation, spreadsheets, presentations\n",
            "* Communication: email, video conferencing, social media\n",
            "* Entertainment: movies, music, games\n",
            "* Education: online learning, research, presentations\n",
            "* Business: remote work, presentations, data analysis\n",
            "* Travel: staying connected while on the move\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Experiment with prompts ***"
      ],
      "metadata": {
        "id": "Zf0ZslVHLHlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fact_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a knowledgeable assistant. Answer this factual question:\\n{question}\"\n",
        ")\n",
        "story_template = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"You are a active storyteller. Write a short story about the following topic:\\n{topic}\"\n",
        ")\n",
        "\n",
        "question = \"Tell me a short story about a spiderman?.\"\n",
        "response = LLMChain(llm=llm, prompt=story_template).run({\"topic\": question})\n",
        "print(\"Story Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "g3X18XA9LcUv",
        "outputId": "01f9afce-52dc-4444-d88f-d33bb7034978"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Story Response: In the bustling metropolis of New York City, amidst the towering skyscrapers and vibrant streets, resided the extraordinary Spider-Man. Peter Parker, an unassuming teenager by day, possessed the astonishing abilities of a spider, bestowed upon him by a radioactive arachnid bite.\n",
            "\n",
            "One fateful evening, as Peter swung through the city, his spider-sense tingled, warning him of danger. He followed the faint vibrations to a dilapidated warehouse on the outskirts of town. Cautiously, he approached the shadowy building, his heart pounding with anticipation.\n",
            "\n",
            "Inside, Spider-Man's senses were overwhelmed by the stench of decay and the eerie silence. As he ventured deeper into the warehouse, he stumbled upon a sight that sent shivers down his spine.\n",
            "\n",
            "A group of masked figures, their bodies covered in black leather, were holding a young woman captive. Their leader, a towering brute with a metallic mask, held a knife to her throat. Fear and desperation filled the woman's eyes as she pleaded for help.\n",
            "\n",
            "Without hesitation, Spider-Man leapt into action. He shot a web at the leader's mask, temporarily blinding him. Then, with lightning-fast reflexes, he disarmed the thugs and freed the woman from their clutches.\n",
            "\n",
            "As the masked figures retreated into the darkness, Spider-Man turned his attention to the woman he had saved. Her name was Anya, and she had been kidnapped by the gang for ransom.\n",
            "\n",
            "Together, they escaped the warehouse and made their way back to safety. As they walked, Anya shared her story with Spider-Man. She was a scientist who had developed a groundbreaking serum that had the potential to cure a deadly disease. The gang had targeted her, hoping to extort money from her research.\n",
            "\n",
            "Spider-Man was moved by Anya's bravery and determination. He vowed to protect her and ensure that her life-saving discovery would reach the world.\n",
            "\n",
            "And so, the web-slinger and the scientist forged an unlikely bond, united by their shared belief in justice and the power of knowledge. Together, they faced the challenges that lay ahead, knowing that with Spider-Man by her side, Anya's dream of a better world would never be extinguished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Add Money :-***"
      ],
      "metadata": {
        "id": "BldM0IHjLz_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-experimental -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAwuZDf1L8Qq",
        "outputId": "6e7f5cf2-5f5c-4c17-ea91-0fc27a8a8f14"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/2.5 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "response1 = conversation.run(\"What is gotham city?\")\n",
        "print(\"Q1:\", response1)\n",
        "\n",
        "response2 = conversation.run(\"Can you explain it in simple way?\")\n",
        "print(\"Q2:\", response2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "IxjqO7XZMKdl",
        "outputId": "4d8a6bbe-5e52-492e-d88e-fbcfe54d9277"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-02c8ffe63113>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "<ipython-input-52-02c8ffe63113>:6: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1: Gotham City is a fictional city appearing in American comic books published by DC Comics, commonly associated with the superhero Batman. It is typically depicted as a dark and corrupt urban environment with a high crime rate and a large population of criminals. The city is often portrayed as being located in the state of New Jersey, across the bay from Metropolis, the home of Superman. It is typically depicted as being a dark and corrupt urban environment with a high crime rate and a large population of criminals.\n",
            "Q2: Sure. Gotham City is a fictional place where Batman, a famous superhero, lives. It's like a city in a comic book, not a real place you can visit. It's often shown as a dark and dangerous place with lots of bad guys, but Batman protects the city and its people from them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Explore Gemini Features:-***"
      ],
      "metadata": {
        "id": "KukzC5wrMice"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = GeminiLLM(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
        "    temperature=0.7,\n",
        "    max_output_tokens=150\n",
        ")"
      ],
      "metadata": {
        "id": "d83tzDFpMkr_"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Integrate Tools:-***"
      ],
      "metadata": {
        "id": "W4YQvvM1MuPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = GeminiLLM(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
        "    temperature=0.7,\n",
        "    max_output_tokens=150\n",
        ")\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer this question:\\n{question}\"\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "conversation = LLMChain(llm=llm, prompt=template, memory=memory)"
      ],
      "metadata": {
        "id": "36NJoVecNWoK"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response1 = conversation.run({\"question\": \"who is the founder of ai?\"})\n",
        "print(response1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "cKfx3Cq_NcP4",
        "outputId": "9d637c0e-e962-45a4-bed3-91504bf47232"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The concept of artificial intelligence (AI) has existed for centuries, but the term was first coined by John McCarthy in 1956. McCarthy is considered to be the founder of AI, and he is credited with developing the first AI program, the Logic Theorist, in 1955.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = conversation.run({\"question\": \"from which year they start study of ai?.\"})\n",
        "print(response2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "v53OF2cpNp9O",
        "outputId": "2b51965d-26cf-4d4a-efaa-a5c32ca66082"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The study of artificial intelligence (AI) began in the mid-20th century, with the Dartmouth Conference in 1956 being widely considered as the starting point.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response3 = conversation.run({\"question\": \"which is the powerful ai tool?\"})\n",
        "print(response3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "G0XyMT0kLw6H",
        "outputId": "8237b30c-95e0-4b10-9604-386c029be1b9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**ChatGPT** is a powerful AI tool.\n",
            "\n",
            "It is a large language model that is trained on a massive dataset of text and code. This allows it to understand and generate human-like text, answer questions, and write different kinds of creative content. ChatGPT can also be used for a variety of other tasks, such as summarizing text, translating languages, and writing different kinds of code.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response4 = conversation.run({\"question\": \"how GPT-3 is powerful?\"})\n",
        "print(response4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "jMjAdRYXN9Mn",
        "outputId": "a50af442-b804-4929-a59f-bd9f4bab9d1d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**GPT-3's Power Stems from Several Key Capabilities:**\n",
            "\n",
            "**1. Large-Scale Language Processing:**\n",
            "* GPT-3 has been trained on an enormous dataset of text and code, giving it a comprehensive understanding of language.\n",
            "* It can generate human-like text, translate languages, summarize documents, and answer questions in a conversational manner.\n",
            "\n",
            "**2. Contextual Understanding:**\n",
            "* GPT-3 excels at understanding the context of input, enabling it to produce relevant and coherent responses.\n",
            "* It considers the history of a conversation, the tone of language, and the speaker's intent.\n",
            "\n",
            "**3. Generative Capabilities:**\n",
            "* GPT-3 can generate original text, code, music, and images from scratch.\n",
            "* This feature has applications in creative writing, natural language processing, and machine learning.\n",
            "\n",
            "**4. Multimodal Integration:**\n",
            "* GPT-3 can process and generate content across various modalities, including text, code, images, and audio.\n",
            "* This allows for seamless integration in applications that require handling diverse data types.\n",
            "\n",
            "**5. Scalability and Performance:**\n",
            "* GPT-3 is deployed on massive computing infrastructure, enabling it to handle large workloads and provide real-time responses.\n",
            "* Its parallelized architecture allows for efficient and scalable training and inference.\n",
            "\n",
            "**6. Adaptability and Fine-tuning:**\n",
            "* GPT-3 can be fine-tuned on specific datasets or tasks to enhance its performance.\n",
            "* This versatility makes it suitable for a wide range of applications, from customer service chatbots to medical diagnosis tools.\n",
            "\n",
            "**7. Continuous Learning and Improvement:**\n",
            "* GPT-3 is constantly being trained and updated with new data, ensuring its knowledge base remains current.\n",
            "* This ongoing learning process enables it to retain its accuracy and relevance over time.\n",
            "\n",
            "**8. Cloud-Based Accessibility:**\n",
            "* GPT-3 is accessible through cloud-based APIs, making it easy for developers to integrate it into their applications and services.\n",
            "* This accessibility fosters innovation and promotes the adoption of AI-powered solutions.\n"
          ]
        }
      ]
    }
  ]
}